{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(requests.get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns true if the response seems to be HTML, false otherwise\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategory():\n",
    "    category = []\n",
    "    text = []\n",
    "\n",
    "    page1 = simple_get('https://www.indonetwork.co.id/categories')\n",
    "    soup1 = BeautifulSoup(page1, 'html.parser')\n",
    "    findCategory = soup1.find_all('div', class_='sub1cat')\n",
    "\n",
    "    for a in findCategory:\n",
    "        href = a.find('a', href=True)\n",
    "        category.append(href['href'])\n",
    "        text.append(href.text)\n",
    "        \n",
    "    return category,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLinkProvince(category):\n",
    "    province = []\n",
    "    checked = True\n",
    "    \n",
    "    categoryPage = simple_get(category+ '/perusahaan')\n",
    "    while(checked):\n",
    "        try:\n",
    "            checked = False\n",
    "            soup2 = BeautifulSoup(categoryPage, 'html.parser')\n",
    "            divProv = soup2.find('div', class_='filkat-sub')\n",
    "            temp = divProv.find_all('a', href=True)\n",
    "        except Exception:\n",
    "            checked = True\n",
    "\n",
    "    for a in temp:\n",
    "\n",
    "        province.append(a['href'])\n",
    "    \n",
    "    \n",
    "    return province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTextProvince(category):\n",
    "    textProv = []\n",
    "    \n",
    "    categoryPage = simple_get(category + '/perusahaan')\n",
    "    soup2 = BeautifulSoup(categoryPage, 'html.parser')\n",
    "    divProv = soup2.find('div', class_='filkat-sub')\n",
    "    temp = divProv.find_all('a', href=True)\n",
    "\n",
    "    for a in temp:\n",
    "\n",
    "        textProv.append(a.text)\n",
    "    \n",
    "    return textProv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkCompany(province):\n",
    "    \n",
    "    linkCompany = []\n",
    "    checked = True\n",
    "    \n",
    "    for page in range(1,10):\n",
    "        \n",
    "        if page == 1:\n",
    "            url = province\n",
    "        else:\n",
    "            url = province + '?page=' + str(page)\n",
    "    \n",
    "        try:\n",
    "            perusahaanPage = simple_get(url)\n",
    "            soup3 = BeautifulSoup(perusahaanPage, 'html.parser')\n",
    "            listCompany = soup3.find_all('div', class_='list-item-company')\n",
    "        except Exception:\n",
    "            try:\n",
    "                perusahaanPage = simple_get(url)\n",
    "                soup3 = BeautifulSoup(perusahaanPage, 'html.parser')\n",
    "                listCompany = soup3.find_all('div', class_='list-item-company')\n",
    "            except Exception:\n",
    "                try:\n",
    "                    perusahaanPage = simple_get(url)\n",
    "                    soup3 = BeautifulSoup(perusahaanPage, 'html.parser')\n",
    "                    listCompany = soup3.find_all('div', class_='list-item-company')\n",
    "                except Exception:\n",
    "                    print('error')\n",
    "                    continue\n",
    "\n",
    "        for i in listCompany:\n",
    "            productInfo = i.find('a', class_='link_product')\n",
    "            if productInfo == None:\n",
    "                continue\n",
    "            else :\n",
    "                linkCompany.append('https:'+ productInfo['href'])\n",
    "\n",
    "    return linkCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinkWhatsapp(url):\n",
    "    \n",
    "    linkWa = []\n",
    "    \n",
    "    driver = webdriver.Chrome('C:/Users/Swift3/Downloads/Scrapper/path/chromedriver')\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        driver.find_element_by_class_name('wa-call').click()\n",
    "    except NoSuchElementException as exception:\n",
    "        return None\n",
    "   \n",
    "    time.sleep(3)\n",
    "    page_source = driver.page_source\n",
    "    soup4 = BeautifulSoup(page_source, 'html.parser').find_all('a', class_='nobor')\n",
    "    for i in soup4:\n",
    "        linkWa.append(i['href'])\n",
    "    \n",
    "    return linkWa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhoneWA(url):\n",
    "    try:\n",
    "        whatsapp = simple_get(url)\n",
    "        soup1 = BeautifulSoup(whatsapp, 'html.parser')\n",
    "        findNumber = soup1.find('span', class_='').text\n",
    "    except Exception:\n",
    "        try:\n",
    "            whatsapp = simple_get(url)\n",
    "            soup1 = BeautifulSoup(whatsapp, 'html.parser')\n",
    "            findNumber = soup1.find('span', class_='').text\n",
    "        except Exception:\n",
    "            try:\n",
    "                whatsapp = simple_get(url)\n",
    "                soup1 = BeautifulSoup(whatsapp, 'html.parser')\n",
    "                findNumber = soup1.find('span', class_='').text\n",
    "            except Exception:\n",
    "                try:\n",
    "                    whatsapp = simple_get(url)\n",
    "                    soup1 = BeautifulSoup(whatsapp, 'html.parser')\n",
    "                    findNumber = soup1.find('span', class_='').text\n",
    "                except Exception:\n",
    "                    return None\n",
    "    \n",
    "    return findNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = getCategory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0\n",
    "\n",
    "for i in range(1, 10):\n",
    "    linkProv = filterLinkProvince(category[0][z+ i])\n",
    "    textProv = filterTextProvince(category[0][z+ i])\n",
    "    \n",
    "    linkCompany = {}\n",
    "    \n",
    "    for x in range(len(textProv)):\n",
    "        linkCompany[textProv[x]] = getLinkCompany(linkProv[x])\n",
    "\n",
    "    for m in textProv:\n",
    "        for l in linkCompany[m]:\n",
    "\n",
    "            print(l)\n",
    "            \n",
    "    data_company = []\n",
    "    phone_wa = []\n",
    "    phone_temp = []\n",
    "\n",
    "    for j in textProv:\n",
    "        for k in linkCompany[j]:\n",
    "            company = simple_get(k)\n",
    "            try:\n",
    "                soup = BeautifulSoup(company, 'html.parser')\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    soup = BeautifulSoup(company, 'html.parser')\n",
    "                except Exception:\n",
    "                    time.sleep(1)\n",
    "                    try:\n",
    "                        soup = BeautifulSoup(company, 'html.parser')\n",
    "                    except Exception:\n",
    "                        print('error')\n",
    "                        continue\n",
    "            companyName = soup.find('h1', class_='sc-company__title').text\n",
    "            membershipLvl = soup.find('span', class_='sc-company__lb').text\n",
    "            companyCtgry = category[1][z + i]\n",
    "            companyDesc = soup.find('div', class_='rc-company__description').text\n",
    "            companyAddr = soup.find('address').text\n",
    "            companyCity = soup.find('span', class_='text-capitalize').text\n",
    "            companyProv = j\n",
    "            linkWA = getLinkWhatsapp(k)\n",
    "            time.sleep(4)\n",
    "            if linkWA is not None:\n",
    "                for wa in linkWA:\n",
    "                    phone_temp.append(getPhoneWA(wa))\n",
    "                if len(phone_temp) == 1:\n",
    "                    phone_temp.append('None')\n",
    "                    phone_temp.append('None')\n",
    "                elif len(phone_temp) == 2:\n",
    "                    phone_temp.append('None')\n",
    "                elif len(phone_temp) == 4:\n",
    "                    del phone_temp[3]\n",
    "                elif len(phone_temp) == 5:\n",
    "                    del phone_temp[3]\n",
    "                    del phone_temp[4]\n",
    "                elif len(phone_temp) == 6:\n",
    "                    del phone_temp[3]\n",
    "                    del phone_temp[4]\n",
    "                    del phone_temp[5]\n",
    "            else:\n",
    "                phone_temp.append('None')\n",
    "                phone_temp.append('None')\n",
    "                phone_temp.append('None')\n",
    "\n",
    "            data_company.append({\n",
    "                'Name':companyName,\n",
    "                'Membership':membershipLvl,\n",
    "                'Category':companyCtgry,\n",
    "                'Description':companyDesc,\n",
    "                'Address':companyAddr,\n",
    "                'City':companyCity,\n",
    "                'Province':companyProv\n",
    "            })\n",
    "\n",
    "            phone_wa.append(phone_temp)\n",
    "\n",
    "            print(companyName)\n",
    "            print(companyCity)\n",
    "            print(companyProv)\n",
    "            print(phone_temp)\n",
    "            print('')\n",
    "            phone_temp = []\n",
    "    \n",
    "    tempData = data_company\n",
    "    for index in range(len(data_company)):\n",
    "        for number in range(len(phone_wa[index])):\n",
    "            tempData[index]['telp' + str(number+1)] = phone_wa[index][number]\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(tempData)\n",
    "    df.to_csv('data_company' + str(z + i + 1) + '.csv', index=False, encoding=\"utf-8\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoryIndoTrading():\n",
    "    category = []\n",
    "    text = []\n",
    "\n",
    "    page1 = simple_get('https://www.indotrading.com/productcatalog/')\n",
    "    soup1 = BeautifulSoup(page1, 'html.parser')\n",
    "    findCategory = soup1.find_all('div', class_='idt-head-catalog cat-product')\n",
    "\n",
    "    for a in findCategory:\n",
    "        href = a.find('a', class_='span-bold clr-red line-clamp-2', href=True)\n",
    "        category.append(href['href'])\n",
    "        text.append(href.text)\n",
    "        \n",
    "    return category,text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainCompanyLink(url):\n",
    "    content = simple_get(url)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    list_company = soup.find('a', class_='idt-elipsis')\n",
    "    listLink = list_company['href']\n",
    "    \n",
    "    return listLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubLinkWithAddress(url):\n",
    "    companyAddress =[]\n",
    "    companyList = []\n",
    "    \n",
    "    for page in range(1, 11):\n",
    "        \n",
    "        if page == 1:\n",
    "            url = url\n",
    "        else:\n",
    "            url = url + str(page)\n",
    "\n",
    "        conte = simple_get(url)\n",
    "        \n",
    "        try:\n",
    "            soup4 = BeautifulSoup(conte, 'html.parser')\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "        addr = soup4.find_all('p', class_='d-flex a-center')\n",
    "        tempList = soup4.find_all('a', class_='span-bold fs-18 product_title pr-10')\n",
    "\n",
    "        for i in range(len(tempList)):\n",
    "            companyList.append(tempList[i]['href'])\n",
    "            companyAddress.append(addr[i].text)\n",
    "    \n",
    "    return companyList, companyAddress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2 = getCategoryIndoTrading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "data_company = []\n",
    "\n",
    "for j in range(1, 100):\n",
    "    mainCompanyLink = getMainCompanyLink(category2[0][j])\n",
    "    tempLink = getSubLinkWithAddress(mainCompanyLink)\n",
    "    \n",
    "    subCompanyLink = tempLink[0]\n",
    "    allAddress = tempLink[1]\n",
    "\n",
    "    for i in range(len(subCompanyLink)):\n",
    "\n",
    "        if allAddress[i] not in temp:\n",
    "            content = simple_get(subCompanyLink[i])\n",
    "            html = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                html.find('div', class_='diamondmember-compro').text\n",
    "                companyMembership = 'Diamond Member'\n",
    "            except Exception:\n",
    "                try:\n",
    "                    html.find('div', class_='platinummember-compro').text\n",
    "                    companyMembership = 'Platinum Member'\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        html.find('div', class_='goldmember-compro').text\n",
    "                        companyMembership = 'Gold Member'\n",
    "                    except Exception:\n",
    "                        companyMembership = 'Tidak Ada'\n",
    "\n",
    "            companyName = html.find('a', class_='mb-5 text--black text--uppercase').text\n",
    "            companyProfile = html.find('div', class_='pb-4').find('div').text\n",
    "            linkWA = html.find('a', class_='hidden realwa mr-0 pr-12')\n",
    "            if linkWA is None:\n",
    "                companyPhone = None\n",
    "            else:\n",
    "                companyPhone = getPhoneWA(linkWA['href'])\n",
    "            companyAddress = allAddress[i]\n",
    "            companyCategory = category2[1][j]\n",
    "            \n",
    "            data_company.append({\n",
    "                'Name':companyName,\n",
    "                'Membership':companyMembership,\n",
    "                'Category':companyCategory,\n",
    "                'Description':companyProfile,\n",
    "                'Address':companyAddress,\n",
    "                'Phone':companyPhone\n",
    "            })\n",
    "            \n",
    "            temp.append(allAddress[i])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data_company)\n",
    "df.to_csv('indo_trading' + '.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = pd.read_csv (r'C:/Users/Swift3/Downloads/Scrapper/ScrappingIndonetwork/indo_trading' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file.to_excel (r'C:/Users/Swift3/Downloads/Scrapper/ScrappingIndonetwork/indo_trading' + '.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
